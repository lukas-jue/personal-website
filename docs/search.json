[
  {
    "objectID": "techacademy.html",
    "href": "techacademy.html",
    "title": "TechAcademy",
    "section": "",
    "text": "Since its founding in 2018, I volunteer at TechAcademy e.V.. After co-leading the organization as an executive board member for several years, my current role as an supervisory board member is to ensure TechAcademy’s long-term viability and successful development.\nTechAcademy e.V. is a student initiative at Goethe University Frankfurt training the next digital leaders. Students of any discipline get the opportunity to acquire coding skills in Data Science and Web Development programs. Students apply their acquired tech skills in projects, solving real-world problems digitally.\nIn addition to regular coding meetups, TechAcademy actively supports students in developing and implementing their projects through workshops, lectures, and company visits. In this way, participants gain practical experience in the tech sector while still studying and establish contact with relevant companies. TechAcademy currently provides more than 100 students per semester with the opportunity to participate in the educational program and counts more than 600 students as alumnae.\nIn addition, TechAcademy organizes the annual TechConference — a digitization conference with high-ranking speakers from business, politics and science.\nFor this voluntary engagement TechAcademy won the national award “Students of the Year 2021,” awarded jointly by the German National Association for Student Affairs (Deutsches Studentenwerk) and the German Association of University Professors and Lecturers (Deutscher Hochschulverband)."
  },
  {
    "objectID": "techacademy.html#videos",
    "href": "techacademy.html#videos",
    "title": "TechAcademy",
    "section": "Videos",
    "text": "Videos\n\nShort Advertisement about Our Program\n\n\n\n\nTechConference 2022, 2021, and 2020"
  },
  {
    "objectID": "techacademy.html#newspaper-articles",
    "href": "techacademy.html#newspaper-articles",
    "title": "TechAcademy",
    "section": "Newspaper Articles",
    "text": "Newspaper Articles\n\nObinja, Antea (October 28, 2020): “Initiative ‚TechAcademy’: Mit Facebook gegen Corona”, in: Frankfurter Allgemeine Zeitung.\nHaerdle, Benjamin (June 4, 2020) “Studenteninitiative: Fit im Programmieren,” in: Süddeutsche Zeitung."
  },
  {
    "objectID": "techacademy.html#press-releases",
    "href": "techacademy.html#press-releases",
    "title": "TechAcademy",
    "section": "Press Releases",
    "text": "Press Releases\n\nTechAcademy (September 21, 2021): “TechConference 2021: With Big Data Comes Big Responsibility,” Press Release.\nGoethe-Universität Frankfurt am Main (September 29, 2021): “8. Oktober / Digitale TechConference zum verantwortungsvollen Umgang mit Big Data”, Press Release.\nDeutsches Studentenwerk (March 18, 2020): “‘TechAcademy’ der Goethe-Universität Frankfurt am Main sind Studierende des Jahres”, Press Release.\nGoethe University Frankfurt am Main (March 18, 2020): “‘Studierende des Jahres’ — TechAcademy ausgezeichnet”, Press Release.\nGoethe University Frankfurt am Main (October 7, 2020): “Digitale TechConference an der Goethe-Universität mit hochkarätigen Teilnehmern”, Press Release.\nTechAcademy e.V. (October 5, 2020): “TechConference: Can Tech Save the World?”, Press Release."
  },
  {
    "objectID": "techacademy.html#more-information",
    "href": "techacademy.html#more-information",
    "title": "TechAcademy",
    "section": "More Information",
    "text": "More Information\nPlease find more information on TechAcademy e.V. on the organization’s website https://tech-academy.io/."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My research investigates economic phenomena in the digital economy. I do so using novel big data sources and applying modern statistical methods.\nOn this site, you will find a summary of my current research projects, grouped by publications and working papers."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\nJürgensmeier, Lukas and Bernd Skiera (2024), “Generative AI for Scalable Feedback to Multimodal Exercises,” International Journal of Research in Marketing, forthcoming, https://doi.org/10.1016/j.ijresmar.2024.05.005.\nProffen, Celina and Lukas Jürgensmeier (2024), “Do political conflicts influence daily consumption choices? Evidence from US-China relations,” Journal of Economic Behavior & Organization, 220, pp. 660-674, https://doi.org/10.1016/j.jebo.2024.02.031, shared first authorship.\nJürgensmeier, Lukas, Jan Bischoff, and Bernd Skiera (2024), “Opportunities for Self-Preferencing in International Online Marketplaces”, International Marketing Review, forthcoming.\nSkiera, Bernd and Lukas Jürgensmeier (2024), “Teaching Marketing Analytics: A Pricing Case Study for Quantitative and Substantive Marketing Skills,” Journal of Marketing Analytics, forthcoming, https://doi.org/10.1057/s41270-024-00313-2.\nSchönborn, Sofie, Lukas Jürgensmeier, Carolin Neumann, Stefan Hildebrand, Gabriel Häusler, David Middelbeck (2023): “Wind of Change: Ehrenamtliche Organisationen und Angebote der jungen Generation zur Digitalbildung in Deutschland,” in: Digitalisierung in der Hochschullehre: Perspektiven und Gestaltungsoptionen, Forum Lehrerinnen- und Lehrerbildung (11). University of Bamberg Press, https://doi.org/10.20378/irb-59190."
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Research",
    "section": "Working Papers",
    "text": "Working Papers\n\nJürgensmeier, Lukas and Bernd Skiera (2024), “Measuring Self-preferencing on Digital Platforms,” working paper available at SSRN (third-round revise-and-resubmit at the Journal of Marketing).\nLukas Jürgensmeier (2024): “keepar: An R Package to Identify, Acquire, and Transform Online Marketplace Data for Social Science Research,” vignette available at lukas-juergensmeier.com/keepar.html package available on GitHub."
  },
  {
    "objectID": "research.html#generative-ai-for-scalable-feedback-to-multimodal-exercises",
    "href": "research.html#generative-ai-for-scalable-feedback-to-multimodal-exercises",
    "title": "Research",
    "section": "Generative AI for Scalable Feedback to Multimodal Exercises",
    "text": "Generative AI for Scalable Feedback to Multimodal Exercises\nJürgensmeier, Lukas and Bernd Skiera (2024), “Generative AI for Scalable Feedback to Multimodal Exercises,” International Journal of Research in Marketing, forthcoming, https://doi.org/10.1016/j.ijresmar.2024.05.005.\nAbstract\nDetailed feedback on exercises helps learners become proficient but is time-consuming for educators and, thus, hardly scalable. This manuscript evaluates how well Generative Artificial Intelligence (AI) provides automated feedback on complex multimodal exercises requiring coding, statistics, and economic reasoning. Besides providing this technology through an easily accessible web application, this article evaluates the technology’s performance by comparing the quantitative feedback (i.e., points achieved) from Generative AI models with human expert feedback for 4,349 solutions to marketing analytics exercises. The results show that automated feedback produced by Generative AI (GPT-4) provides almost unbiased evaluations while correlating highly with (r = 0.94) and deviating only 6 % from human evaluations. GPT-4 performs best among seven Generative AI models, albeit at the highest cost. Comparing the models’ performance with costs shows that GPT-4, Mistral Large, Claude 3 Opus, and Gemini 1.0 Pro dominate three other Generative AI models (Claude 3 Sonnet, GPT-3.5, and Gemini 1.5 Pro). Expert assessment of the qualitative feedback (i.e., the AI’s textual response) indicates that it is mostly correct, sufficient, and appropriate for learners. A survey of marketing analytics learners shows that they highly recommend the app and its Generative AI feedback. An advantage of the app is its subject-agnosticism—it does not require any subject- or exercise-specific training. Thus, it is immediately usable for new exercises in marketing analytics and other subjects.\nKeywords: Generative AI, Automated Feedback, App, Marketing Analytics, Learning."
  },
  {
    "objectID": "research.html#do-political-conflicts-influence-daily-consumption-choices-evidence-from-us-china-relations",
    "href": "research.html#do-political-conflicts-influence-daily-consumption-choices-evidence-from-us-china-relations",
    "title": "Research",
    "section": "Do Political Conflicts Influence Daily Consumption Choices? Evidence from US-China Relations",
    "text": "Do Political Conflicts Influence Daily Consumption Choices? Evidence from US-China Relations\nProffen, Celina and Lukas Jürgensmeier (2024), “Do political conflicts influence daily consumption choices? Evidence from US-China relations,” Journal of Economic Behavior & Organization, 220, pp. 660-674, https://doi.org/10.1016/j.jebo.2024.02.031, shared first authorship.\nAbstract\nDoes political conflict with another country influence domestic consumers’ daily consumption choices? We exploit the volatile US-China relations in 2018 and 2019 to analyze whether US consumers reduce their visits to Chinese restaurants when bilateral relations deteriorate. We measure the degree of political conflict through negativity in media reports and rely on smartphone location data to measure daily visits to over 190,000 US restaurants. A deterioration in US-China relations induces a significant decline in visits not only to Chinese but also to other foreign ethnic restaurants, while visits to typical American restaurants increase. We identify consumers’ age, race, and cultural openness to moderate the strength of this ethnocentric effect.\nKeywords: Political conflict, Consumption, Boycotts, Ethnocentrism"
  },
  {
    "objectID": "research.html#opportunities-for-self-preferencing-in-international-online-marketplaces",
    "href": "research.html#opportunities-for-self-preferencing-in-international-online-marketplaces",
    "title": "Research",
    "section": "Opportunities for Self-Preferencing in International Online Marketplaces",
    "text": "Opportunities for Self-Preferencing in International Online Marketplaces\nJürgensmeier, Lukas, Jan Bischoff, and Bernd Skiera (2024), “Opportunities for Self-Preferencing in International Online Marketplaces”, International Marketing Review, forthcoming.\nAbstract\nLarge digital platforms face intense scrutiny over self-preferencing, which involves a platform provider favoring its own offers over those of competitors. In online marketplaces, also called retail or e-commerce platforms, much of the academic and regulatory debate focuses on determining whether the marketplace provider gives preference to its own private labels, such as “Amazon Basics” or Walmart’s “Great Value” products. However, we outline, both conceptually and empirically, that self-preferencing can also occur through other dimensions of vertical integration—namely, retailing and fulfillment. This article contributes by conceptualizing three dimensions of vertical integration in online marketplaces—private labels, retailing, and fulfillment. Then, two studies empirically assess (i) which of the 20 most-visited global online marketplaces vertically integrate which dimension and (ii) which share of 600 million available offers are vertically integrated to which degree in eleven international Amazon marketplaces. The majority of the leading marketplaces vertically integrate all three dimensions, implying ample opportunities for self-preferencing. Across international Amazon marketplaces, only 0.02% of available offers consist of an Amazon private-label product. However, Amazon is a retailer for around 31% and fulfills around 38% of all available offers in its marketplaces. Hence, self-preferencing on Amazon can occur most frequently through retailing and fulfillment but comparatively infrequently through private-label offers. Still, these shares differ substantially by country—every second offer is vertically integrated in the US, but only one in ten in India. Most of the self-preferencing debate often focuses on private-label products. Instead, we present large-scale empirical results showing that self-preferencing on Amazon could occur most often through retailing and fulfillment because these channels affect much larger shares of offers. We also measure the variation of these shares across countries and relate them to regulatory environments."
  },
  {
    "objectID": "research.html#teaching-marketing-analytics-a-pricing-case-study-for-quantitative-and-substantive-marketing-skills",
    "href": "research.html#teaching-marketing-analytics-a-pricing-case-study-for-quantitative-and-substantive-marketing-skills",
    "title": "Research",
    "section": "Teaching Marketing Analytics: A Pricing Case Study for Quantitative and Substantive Marketing Skills",
    "text": "Teaching Marketing Analytics: A Pricing Case Study for Quantitative and Substantive Marketing Skills\nSkiera, Bernd and Lukas Jürgensmeier (2024), “Teaching Marketing Analytics: A Pricing Case Study for Quantitative and Substantive Marketing Skills,” Journal of Marketing Analytics, forthcoming, https://doi.org/10.1057/s41270-024-00313-2.\nThe case study, solution, and data is available through GitHub.\nAbstract\nThis article describes a data-driven case study for teaching and assessing students’ skills in marketing analytics, specifically in pricing. This case study combines teaching econometrics to analyze data and substantive marketing to derive managerial insights. The econometric challenge requires students to set up and implement a regression analysis to derive the demand function, detect multicollinearity, and select appropriate data visualizations. The substantive challenge requires deriving optimal pricing decisions and understanding how the parameters of the demand function impact optimal prices and the associated profit. We test the case study in a marketing analytics exam and discuss the performance of 134 students. Beyond assessing student performance in an exam, the case study facilitates teaching through in-class group work or assignments. Free of charge, under a liberal CC BY license, we encourage other educators to use the case study in their teaching. We provide the necessary data and a sample solution using the statistical programming language R.\nKeywords: Marketing Analytics, Pricing, Teaching, Education, Case Study, Data Science"
  },
  {
    "objectID": "research.html#non-profit-organizations-and-their-offers-for-digital-education-in-germany",
    "href": "research.html#non-profit-organizations-and-their-offers-for-digital-education-in-germany",
    "title": "Research",
    "section": "Non-Profit Organizations and their Offers for Digital Education in Germany",
    "text": "Non-Profit Organizations and their Offers for Digital Education in Germany\nSchönborn, Sofie, Lukas Jürgensmeier, Carolin Neumann, Stefan Hildebrand, Gabriel Häusler, David Middelbeck (2023): “Wind of Change: Ehrenamtliche Organisationen und Angebote der jungen Generation zur Digitalbildung in Deutschland,” in: Digitalisierung in der Hochschullehre: Perspektiven und Gestaltungsoptionen, Forum Lehrerinnen- und Lehrerbildung (11). University of Bamberg Press, https://doi.org/10.20378/irb-59190.\nAbstract\nIn addition to traditional educational institutions, volunteer organizations play an important role, especially in digital education. This article shows which volunteer-organized offerings exist in Germany, discusses the innovative concepts behind them, and illustrates how traditional educational institutions can learn from this volunteer engagement. To this end, the authors present the results of a survey of 19 organizations and two exemplary projects in case studies, and recommend three concrete actions for education policy based on these results.\nKeywords: Digital Education; Volunteering; Blended Learning; Peer Learning; Coding; Survey"
  },
  {
    "objectID": "research.html#measuring-self-preferencing-on-digital-platforms",
    "href": "research.html#measuring-self-preferencing-on-digital-platforms",
    "title": "Research",
    "section": "Measuring Self-Preferencing on Digital Platforms",
    "text": "Measuring Self-Preferencing on Digital Platforms\nAuthors: Lukas Jürgensmeier and Bernd Skiera.\nStatus: Under third-round review at the Journal of Marketing.\nRead the full working paper on SSRN.\nAbstract\nDigital platforms use recommendations to facilitate exchanges between platform actors, such as trade between buyers and sellers. Aiming to protect consumers and guarantee fair competition on platforms, legislators increasingly require that recommendations on market-dominating platforms be free from self-preferencing. That is, platforms that also act as sellers (e.g., Amazon) or information providers (e.g., Google) must not prefer their own offers over comparable third-party offers. Yet, successful enforcement of self-preferencing bans—to the potential benefit of consumers and third-party actors—requires defining and measuring self-preferencing across a platform. In the context of recommendations through search results, this research contributes by i) conceptualizing a “recommendation” as an offer’s level of search engine visibility across an entire platform (instead of its position in specific search queries, as in previous research); ii) discussing two tests for self-preferencing, and iii) implementing them in two empirical studies across three international Amazon marketplaces. Contrary to consumer expectations and emerging literature, our analysis finds almost no evidence for self-preferencing. A survey reveals that even if Amazon were proven to engage in self-preferencing, most consumers would not change their shopping behavior on the platform—highlighting Amazon’s significant market power and suggesting the need for robust protections for sellers and consumers.\nKeywords: Digital Platforms, Amazon, Competition, Antitrust, Search Engines, Digital Markets Act, American Innovation and Choice Online Act."
  },
  {
    "objectID": "research.html#keepar-an-r-package-to-identify-acquire-and-transform-online-marketplace-data-for-social-science-research",
    "href": "research.html#keepar-an-r-package-to-identify-acquire-and-transform-online-marketplace-data-for-social-science-research",
    "title": "Research",
    "section": "keepar: An R Package to Identify, Acquire, and Transform Online Marketplace Data for Social Science Research",
    "text": "keepar: An R Package to Identify, Acquire, and Transform Online Marketplace Data for Social Science Research\nAuthors: Lukas Jürgensmeier\nStatus: Package available at GitHub; draft working paper available upon request.\nAbstract\nResearchers frequently use online marketplace data for social science research. However, obtaining and processing such data is cumbersome, and either involves web scraping or access to a pre-existing database. One such a database is the commercial data provider Keepa.com, which tracks Amazon products over time. While such database is accessible via API, identifying and processing the relevant data—into a suitable format for social science research—requires significant effort. This article introduces the keepar package, an R package designed to simplify this data acquisition and transformation process. Through an illustrative research project analyzing the market for rubber ducks on Amazon, this article explains how to use the package to obtain and transform the Keepa.com data. Making this package available open source, this article contributes to making this data source more widely available to social science researchers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lukas Jürgensmeier",
    "section": "",
    "text": "I am a Ph.D. Student and Research Associate in Quantitative Marketing at Goethe University Frankfurt. My research and teaching focuses on the interface between Data Science and Marketing. I like applying modern econometric methods to novel data sources in the digital economy—and teach students how to do that with R and Python.\nIn my research project Measuring Self-Preferencing on Digital Platforms, we develop an approach to test for self-preferencing through platform recommendations.\n“Do Political Conflicts Influence Daily Consumption Choices?”—published in the Journal of Economic Behavior and Organization—combines vast smartphone location data and textual analysis of newspaper articles to assess the impact of political conflict on consumption.\nFor the newest project, we created a Generative AI app that provide personalized feedback to students. Published in the International Journal of Research in Marketing, Generative AI for Scalable Feedback to Multimodal Exercises describes how well that works (spoiler: quite well!)\nMy teaching experience includes:\n\nDesigning and teaching Introduction to Data Science courses in R and Python for students from all fields,\nTeaching a Marketing Analytics course that enables students to solve marketing problems with econometrics, programming, and substantive marketing skills,\nTeaching an Empirical Customer Analytics seminar about machine learning methods in marketing analytics, and\nDesigning and teaching TechAcademy’s blended learning Data Science with R courses.\n\nIn my free time, I help preparing the next generation for the digital future as a member of the supervisory board at TechAcademy e.V., a non-profit organization teaching coding and promoting tech literacy.\nOn this website, you will find information about my Academic Research and Teaching, my Curriculum Vitae, and my passion project—TechAcademy e.V.\nYou can contact me via LinkedIn or \n\n\n&lt;span style=\"unicode-bidi:bidi-override;direction:rtl;\"&gt;moc.liamg@reiemsnegreuj.l&lt;/span&gt;"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Lukas Jürgensmeier\nTheodor-W.-Adorno-Platz 4\n60323 Frankfurt am Main\nGermany\n\n\n\nmoc.liamg@reiemsnegreuj.l\n\n\nThis website was built using Quarto — an open-source scientific and technical publishing system built on Pandoc."
  },
  {
    "objectID": "curriculum-vitae.html",
    "href": "curriculum-vitae.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Lukas Jürgensmeier is Ph.D. Student and Research Assistant in Quantitative Marketing at Goethe University. In his free time, Lukas enjoys preparing the next generation for the digital future as a Member of the Board at TechAcademy e.V., a non-profit organization teaching coding and promoting tech literacy."
  },
  {
    "objectID": "curriculum-vitae.html#education",
    "href": "curriculum-vitae.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\n\nDegree\nInstitution\nDate\n\n\n\n\nPh.D. in Quantitative Marketing\nGoethe University, Frankfurt, Germany\n2021 – 2024\n\n\nM.Sc. in Business Administration\nGoethe University, Frankfurt, Germany\n2018 – 2021\n\n\nWashington Semester Program\nAmerican University, Washington, DC, USA\n2017\n\n\nB.Sc. in Business Administration\nUniversity of Augsburg, Germany\n2015 – 2018"
  },
  {
    "objectID": "curriculum-vitae.html#experience",
    "href": "curriculum-vitae.html#experience",
    "title": "Curriculum Vitae",
    "section": "Experience",
    "text": "Experience\nCurrently, I am a Doctoral Student at the Marketing Department of Goethe University, Frankfurt (Chair for E-Commerce, Prof. Dr. Bernd Skiera). Additionally, I am a member of the supervisory board at TechAcademy e.V., a non-profit organization teaching coding and promoting tech literacy.\nMy previous work experiences include:\n\nRevenue Management at Deutsche Lufthansa AG in Munich,\nFinance & Investor Relations at Frankfurt airport (Fraport AG), and\nInt’l Government & Regulatory Affairs at CompTIA (Computing Technology Industry Association) in Washington, DC.\n\nPlease see my LinkedIn profile for more information."
  },
  {
    "objectID": "curriculum-vitae.html#publications",
    "href": "curriculum-vitae.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\n\nJürgensmeier, Lukas and Bernd Skiera (2024), “Generative AI for Scalable Feedback to Multimodal Exercises,” International Journal of Research in Marketing, forthcoming, https://doi.org/10.1016/j.ijresmar.2024.05.005.\nProffen, Celina and Lukas Jürgensmeier (2024), “Do political conflicts influence daily consumption choices? Evidence from US-China relations,” Journal of Economic Behavior & Organization, 220, pp. 660-674, https://doi.org/10.1016/j.jebo.2024.02.031, shared first authorship.\nJürgensmeier, Lukas, Jan Bischoff, and Bernd Skiera (2024), “Opportunities for Self-Preferencing in International Online Marketplaces”, International Marketing Review, forthcoming.\nSkiera, Bernd and Lukas Jürgensmeier (2024), “Teaching Marketing Analytics: A Pricing Case Study for Quantitative and Substantive Marketing Skills,” Journal of Marketing Analytics, forthcoming, https://doi.org/10.1057/s41270-024-00313-2.\nSchönborn, Sofie, Lukas Jürgensmeier, Carolin Neumann, Stefan Hildebrand, Gabriel Häusler, David Middelbeck (2023): “Wind of Change: Ehrenamtliche Organisationen und Angebote der jungen Generation zur Digitalbildung in Deutschland,” in: Digitalisierung in der Hochschullehre: Perspektiven und Gestaltungsoptionen, Forum Lehrerinnen- und Lehrerbildung (11). University of Bamberg Press, https://doi.org/10.20378/irb-59190."
  },
  {
    "objectID": "curriculum-vitae.html#working-papers",
    "href": "curriculum-vitae.html#working-papers",
    "title": "Curriculum Vitae",
    "section": "Working Papers",
    "text": "Working Papers\n\nJürgensmeier, Lukas and Bernd Skiera (2024), “Measuring Self-preferencing on Digital Platforms,” working paper available at SSRN (third-round revise-and-resubmit at the Journal of Marketing).\nLukas Jürgensmeier (2024): “keepar: An R Package to Identify, Acquire, and Transform Online Marketplace Data for Social Science Research,” vignette available at lukas-juergensmeier.com/keepar.html package available on GitHub.\n\nPlease see Research for detailed information."
  },
  {
    "objectID": "curriculum-vitae.html#research-grants-and-fundraising",
    "href": "curriculum-vitae.html#research-grants-and-fundraising",
    "title": "Curriculum Vitae",
    "section": "Research Grants and Fundraising",
    "text": "Research Grants and Fundraising\n\nResearch Grant: Do Political Conflicts Influence Daily Consumption Choices?, ~ 25,000 € provided by the efl — the Data Science Institute. Jointly with Celina Proffen\nTeaching Grant: DigiTeLL 2023: Coding Intro — A Scalable Concept for Introductory Coding Courses for Students from all Fields, ~ 80,000 € provided by Stiftung Innovation in der Hochschullehre (Foundation for Innovation in University Education) and Goethe University. Jointly with Bernd Skiera.\nTeaching Grant: DigiTeLL 2022: Flipped Classroom and Gamification in Marketing Analytics Education, ~ 80,000 € provided by Stiftung Innovation in der Hochschullehre (Foundation for Innovation in University Education) and Goethe University. Jointly with Bernd Skiera.\nNon-Profit Fundraising: TechAcademy e.V., during my tenure we raised more than 130,000 € through corporate sponsorship and prize money to provide free coding education and finance the annual TechConference. Sponsors include Meta, Salesforce, ING, Bearing Point, Huawei, zeb, and Lidl."
  },
  {
    "objectID": "curriculum-vitae.html#honors-awards",
    "href": "curriculum-vitae.html#honors-awards",
    "title": "Curriculum Vitae",
    "section": "Honors & Awards",
    "text": "Honors & Awards\n\nISMS Doctoral Dissertation Award 2023\nAwarded for my dissertation proposal Measuring Fair Competition by the INFORMS Society for Marketing Science (ISMS), $5,000 prize money.\nPlease access the ISMS website for more information on the ISMS Doctoral Dissertation Award.\n\n\nErich-Gutenberg-Award 2021\nAwarded for my Master’s Thesis Measuring Fair Competition on Digital Platforms: Visibility as a Novel Metric in Antitrust Cases, 1,000 € prize money.\nThe Erich-Gutenberg-Arbeitsgemeinschaft honors a “qualitatively remarkable theses on the solution of decision problems in companies” once a year. In the thesis, supervised by Prof. Dr. Bernd Skiera, I develop and implement an approach to identify unfair treatment in search results. The thesis contributes to the current debate on whether digital platforms abuse their market power. With my systematic and data-driven approach, regulators and businesses can test, if digital platforms such as Amazon provide fair search results — especially if the offers of the platform are in direct competition with the offers of third parties.\nMarketing Department, Goethe University (November 28, 2021): Lukas Jürgensmeier receives Erich Gutenberg Prize for his Thesis, Press Release.\n\n\nStudent of the Year 2021\nAwarded for my work at TechAcademy e.V. by the German National Association for Student Affairs (Deutsches Studentenwerk) and the German Association of University Professors and Lecturers (Deutscher Hochschulverband), 5,000 € prize money.\n\nThe award recognizes students who demonstrate outstanding commitment above and beyond their academic achievements, which should be as unique and innovative as possible. In the unanimous opinion of the six-member DHV/DSW jury, the TechAcademy meets these criteria through its exceptional and exemplary commitment in every respect.\n\n\n“With its initiative, TechAcademy contributes to enabling students from less computer science-related courses of study, in particular, to get started in programming. TechAcademy addresses students in a field that is often closed to many students. This interdisciplinary, self-developed offering for students of all disciplines is exemplary, and it is a lighthouse project for the educationally central topic of digital education. It points the way to the future for self-initiated learning, for the promotion of IT skills through a sense of community,” said DHV President Prof. Dr. Bernhard Kempen and DSW President Prof. Dr. Rolf-Dieter Postlep, jointly explaining the award.\n\nDeutsches Studentenwerk (March 18, 2020): “‘TechAcademy’ der Goethe-Universität Frankfurt am Main sind Studierende des Jahres”, Press Release.\nGoethe University Frankfurt am Main (March 18, 2020): “‘Studierende des Jahres’ — TechAcademy ausgezeichnet”, Press Release.\n\n\nNorbert-Walter-Prize 2019\nFor the best Master’s-level seminar theses in business and economics at Goethe University for my paper “Un-Black-Boxing Artificial Neural Networks: Predicting and Explaining Bank Customer’s Cross-Sell Likelihood” (second price). Awarded by the Frankfurter Wirtschaftswissenschaftliche Gesellschaft (fwwg), 400 € prize money.\n\n\nSchmalenbach Scholarship (2019 – 2021)\nFor outstanding students in a Master’s program in Business. Awarded based on academic qualification and social engagement by the Schmalenbach Gesellschaft für Betriebswirtschaft e.V."
  },
  {
    "objectID": "keepar.html",
    "href": "keepar.html",
    "title": "keepar",
    "section": "",
    "text": "Researchers frequently use online marketplace data for social science research. However, obtaining and processing such data is cumbersome and often involves web scraping or access to a pre-existing database. One such database is the commercial data provider Keepa, which tracks Amazon products and sellers over time. While this database is accessible via API, identifying, acquiring, and transforming the data into a suitable format for social science research requires significant effort. This article introduces the keepar package, an R package designed to simplify working with this Amazon marketplace data. Through an illustrative research project analyzing the market for rubber ducks on Amazon, this article explains how to use the package to work with Keepa data. By making the corresponding R package open source and available on GitHub, this article aims to facilitate further use of this rich data source in social science research."
  },
  {
    "objectID": "keepar.html#aim-facilitating-research-into-online-marketplaces-by-introducing-the-keepar-package-through-an-illustrative-research-project",
    "href": "keepar.html#aim-facilitating-research-into-online-marketplaces-by-introducing-the-keepar-package-through-an-illustrative-research-project",
    "title": "keepar",
    "section": "1.1 Aim: Facilitating Research into Online Marketplaces by Introducing the keepar Package Through an Illustrative Research Project",
    "text": "1.1 Aim: Facilitating Research into Online Marketplaces by Introducing the keepar Package Through an Illustrative Research Project\nThis article aims to show how social science researchers can use the keepar package to identify, acquire, and transform Amazon marketplace data. To this end, the article seeks to facilitate use of both the package and data by providing and discussing R code for all steps required to implement this analysis—from data acquisition to data transformation to model estimation.\nI do so by illustrating a mock research project analyzing the market for rubber ducks. Ultimately, this article shows how researchers can transform the acquired data for use in standard econometric methods. The empirical analysis implements a simple differences-in-differences analysis to estimate how much the sales of Christmas-themed rubber ducks increase during the Christmas season compared to all other rubber ducks.\nWhen consumers search for “rubber duck” on Amazon, they can choose from many different products. These rubber ducks vary in design, price, and ratings. Figure 1 shows some examples of rubber ducks on the first page of the search results—but that is only the beginning. As the subsequent analysis reveals, Amazon lists hundreds of rubber ducks on the marketplace. Using the illustrative example of these little rubber ducks, this article will walk through the functions of the keepar package. While this rubber duck exercise may initially seem a bit silly, it mirrors the relevant steps a social sciences researcher would take when acquiring and analyzing online marketplace data.\n\n\n\nFigure 1: A Selection of Rubber Ducks on Amazon"
  },
  {
    "objectID": "keepar.html#requirement-a-tidy-panel-data-set-of-rubber-ducks",
    "href": "keepar.html#requirement-a-tidy-panel-data-set-of-rubber-ducks",
    "title": "keepar",
    "section": "1.2 Requirement: A Tidy Panel Data Set of Rubber Ducks",
    "text": "1.2 Requirement: A Tidy Panel Data Set of Rubber Ducks\nAgain, this article aims to facilitate research into online marketplaces by providing an accessible package for most aspects of data identification, acquisition, and preparation for Amazon marketplace data. Before obtaining the raw data, let us define which data structure we require to fulfill this aim. Many standard econometric methods require a tidy panel data set that includes the relevant product information—for example, daily historical price data. What exactly does that mean? We want to create a data set that is\n\ntidy by following the tidy data principle (Wickham et al. 2019). Most importantly, every column will represent a variable, and every row will represent a single observation of all variables;\na panel, meaning it includes the historical data of multiple products. Specifically, this means that we have a variable identifying the product (the ASIN—Amazon Standard Identification Number) and one variable identifying the time (often, it will be a date for a daily panel frequency). The combination of ASIN \\(\\times\\) date will then uniquely identify each observation, i.e., each row.\n\nEventually, the data set should look like Table 1, which illustrates the above principles.\n\n\n\nTable 1: Tidy Panel Data Set\n\n\n\n\n\nASIN\nDate\nPrice\nSales Rank\n…\n\n\n\n\nASIN-Rubber-Duck-A\ntoday\n9.99\n2\n\n\n\nASIN-Rubber-Duck-A\nyesterday\n8.99\n1\n\n\n\nASIN-Rubber-Duck-B\ntoday\n12.49\n1\n\n\n\nASIN-Rubber-Duck-B\nyesterday\n14.99\n2\n\n\n\n…\n\n\n\n\n\n\n\n\n\n\nThis data structure will allow us to work easily with the data in various ways—be it visualization through ggplot2, data wrangling with dplyr, or regression modeling with fixest. Before estimating sophisticated models, however, we must first identify, acquire, and transform the data.\nThe remainder of this article walks the reader through these steps. The following section first outlines how to use keepa’s functions to identify, acquire, and transform the data. Then, the article shows how to use this prepared data to conduct an illustrative research project. The last section includes a summary, conclusions, and an outlook for further research."
  },
  {
    "objectID": "keepar.html#preparation",
    "href": "keepar.html#preparation",
    "title": "keepar",
    "section": "2.1 Preparation",
    "text": "2.1 Preparation\n\n2.1.1 API Access and Workspace Setup\nWhile Keepa offers a free but restricted account, we will require a paid subscription for API access. Currently, the cheapest subscription includes limited access to the API. Depending on how much data you need to download (and how quickly), you might need a more expensive subscription. You can read about the available plans at Keepa.com/#!api. I recommend starting with the cheapest subscription and upgrading if necessary.\nBefore we start, please store your private API key. A convenient way is to add this API key as a system environment variable in the .RProfile file. You can use the following code to open the file:\n\nfile.edit(\"~/.Rprofile\")\n\nThen, add Sys.setenv(\"KEEPA_API_KEY\" = \"insert your private Keepa API Key here\") to the .RProfile file. Finally, load the API key into your workspace by running:\n\napi_key &lt;- Sys.getenv(\"KEEPA_API_KEY\")\n\nLet us start by loading the keepar package. Before loading it, you must install the package once on your machine by running devtools::install_github(\"lukas-jue/keepar\"). (You also need the package devtools, which you can install through install.packages(\"devtools\").)\n\nlibrary(keepar)\n\nWhile this article introduces the most common functions, you can access the package’s complete documentation, including all available functions, by running the command help(package = keepar).\nYou can test whether your API access works by checking your API account’s status:\n\ncheck_status(api_key)\n#&gt; # A tibble: 1 x 7\n#&gt;   timestamp           tokensLeft refillIn refillRate tokenFlowReduction\n#&gt;   &lt;dttm&gt;                   &lt;int&gt; &lt;Period&gt;      &lt;int&gt;              &lt;dbl&gt;\n#&gt; 1 2024-07-09 10:23:59        300 37.861S           5                  0\n#&gt; # i 2 more variables: tokensConsumed &lt;int&gt;, processingTimeInMs &lt;int&gt;\n\nEvery API call costs tokens. The greater the number of products or the more detailed the requested data is, the more tokens it will consume. Calling the API to check its status through check_status does not consume any tokens and returns tokensLeft. This value indicates how many tokens are currently available on your account.\nIf your token balance becomes negative, you can only make new requests once the balance becomes positive again. Figuring out when this will occur takes some calculations. The following function implements this functionality:\n\ncheck_sufficient_tokens(api_key)\n#&gt; # A tibble: 1 x 3\n#&gt;   tokensBalance secondsUntilTokensPositive datetimeTokensPositive\n#&gt;           &lt;int&gt;                      &lt;dbl&gt; &lt;dttm&gt;                \n#&gt; 1           300                          0 2024-07-09 12:23:59\n\n\n\n2.1.2 Load Packages and Set Up Project Structure\nIn addition to the previously loaded keepar package, please load the tidyverse package for visualization and further data processing. Because the data can become voluminous, we will use the fast and highly efficient arrow package to save and read the data in the parquet format. Since we will work with many files and paths, we will also load the fs package to simplify this work.\n\nlibrary(tidyverse)\nlibrary(arrow)\nlibrary(fs)\n\nI suggest setting up an R project, as recommended by Bryan (2017). Using a project-oriented workflow enables a self-contained project that integrates easily into version control systems and is executable on the local machines of collaborators or others aiming to replicate or extend your analysis. Within this project, please create a folder data, into which we will download and transform the data."
  },
  {
    "objectID": "keepar.html#identify-relevant-products",
    "href": "keepar.html#identify-relevant-products",
    "title": "keepar",
    "section": "2.2 Identify Relevant Products",
    "text": "2.2 Identify Relevant Products\nFirst, we want to identify all relevant available rubber ducks on Amazon. But what does all relevant mean? How can we filter for specific products? This section explains and illustrates this selection process. As a first result, we want to count the number of all relevant available rubber ducks in the database.\n\n2.2.1 Using the Product Finder to Obtain Product ASINs\nOur journey starts on the Keepa website, where we use Keepa’s product finder to identify the products whose data we want to download. The product finder lets users filter the database to identify the relevant products. For example, you can filter products from a specific category, within specific price ranges, or offered by particular sellers. After setting the desired filters, we can use this tool to generate a .json file that contains all filters. Later, we can send this .json along with our API request to receive the product identifier—referred to as an ASIN.\nAfter defining the relevant filters, click SHOW API QUERY in the product finder. You will then see a .json file similar to this one:\n\n{\n    \"title\": \"rubber duck\",\n    \"trackingSince_lte\": 4733220, \n    \"categories_include\": [\n        \"330390011\" \n    ],\n     \"productType\": [\n        \"0\" \n    ],\n    \"hasParentASIN\": false, \n    \"sort\": [\n        [\n            \"current_SALES\", \n            \"asc\"\n        ]\n    ],\n    \"perPage\": 100, \n    \"page\": 0 \n}\n\nSave this .json file in your project directory. I recommend you name it data/1-getASIN/product-finder-api-query.json.\n\n\n2.2.2 Calling the Product Finder API to Obtain the ASINs\nYou can obtain the ASINs matching your filter conditions by sending the .json file as a payload to the API. First, you read the .json file into your environment.\n\npayload &lt;- readr::read_file(\n  \"data/1-getASIN/product-finder-api-query.json\"\n  )\n\nSecond, define the marketplace for which you want to get to get ASINs and call the API through get_product_asin().\n\ndomain_id &lt;- 1\nproduct_asin &lt;- get_product_asin(api_key, domain_id, payload)\n\nThis call will yield a data frame containing the first 100 ASINs that match your filter criteria (in ascending order by sales rank, i.e., the best-selling product comes first).\n\nproduct_asin\n#&gt; # A tibble: 100 x 1\n#&gt;    asin      \n#&gt;    &lt;chr&gt;     \n#&gt;  1 B07L6BBW2Q\n#&gt;  2 B07PB9GVPT\n#&gt;  3 B07SMV3L4V\n#&gt;  4 B007FPX8Z2\n#&gt;  5 B01DW6S72Q\n#&gt;  6 B07GMSSXN5\n#&gt;  7 B008C2F0HQ\n#&gt;  8 B00DS4EY4S\n#&gt;  9 B07HCQTDK7\n#&gt; 10 B00ILCA9G4\n#&gt; # i 90 more rows\n\nNow, save this file to the pre-defined folder. Saving the ASINs will make your research more reproducible; since Keepa’s database (and the underlying sales ranks on Amazon) update continuously, re-running the same code later will yield a different set of ASINs.\n\nwrite_parquet(product_asin, \"data/1-getASIN/asin-page-0.parquet\")\n\nIf you only need up to 100 products, you can skip the following paragraphs and jump to the next subsection.\nHowever, you will often want to obtain more than the first 100 ASINs. In this case, we must send multiple requests by iterating through the page variable in the .json file. Here is how you can do that:\n\npages &lt;- 1:2 # pages 2 and 3\n\nfor (i in pages) {\n  payload_edited &lt;- payload |&gt;\n    str_replace(\"page\\\\\\\": 0\", paste0(\"page\\\\\\\": \", i))\n  product_asin &lt;- get_product_asin(\n    api_key, domain_id, payload_edited\n    )\n  write_parquet(\n    product_asin, paste0(\"data/1-getASIN/asin-page-\", i, \".parquet\")\n    )\n}\n\nNote that, unlike the programming language R, the API request starts counting from 0. So the second page is 1 instead of 2. We number the files accordingly, starting from 0.\nOnce you have saved multiple ASIN files, we want to re-read them into our workspace and bind them together to have a list of all ASINs. First, we identify all files on the disk which we want to read in:\n\nvec_path_asin &lt;- dir_ls(\"data/1-getASIN/\") |&gt; str_subset(\"parquet\")\nvec_path_asin\n#&gt; [1] \"data/1-getASIN/asin-page-0.parquet\" \"data/1-getASIN/asin-page-1.parquet\"\n#&gt; [3] \"data/1-getASIN/asin-page-2.parquet\"\n\nThen, we read them all at once with the help of purrr::map_dfr:\n\ndf_asin &lt;- map_dfr(vec_path_asin, read_parquet)\n\nAfter reading the data, we can finally answer the initial question: How many rubber ducks are available? We selected quite a narrow set of products with 247 rubber ducks, each represented by its ASIN.\n\nnrow(df_asin)\n#&gt; [1] 247\n\nHowever, obtaining the relevant ASINs for our products of interest is only the first step. We now want to analyze the product characteristics."
  },
  {
    "objectID": "keepar.html#obtaining-raw-product-data",
    "href": "keepar.html#obtaining-raw-product-data",
    "title": "keepar",
    "section": "2.3 Obtaining Raw Product Data",
    "text": "2.3 Obtaining Raw Product Data\nOnce we have identified products for which we want to obtain detailed information—by obtaining their ASINs—we can query Keepa’s database to return such information.\nFirst, we create batches of up to 100 ASINs because the Keepa API will only return information on a maximum of 100 products for one API request.\n\nlst_product_batches &lt;- \n  df_asin |&gt; \n  mutate(batch_id = (row_number() - 1) %/% 100) |&gt; \n  group_by(batch_id) |&gt; \n  nest() |&gt;\n  pull(data)\n\nThe newly generated lst_product_batches is a list of data frames that include a maximum of 100 ASINs. Now, we can query the Keepa API for detailed data on each of these ASINs.\n\n2.3.1 Requesting Data for up to 100 Products\nFirst, we build the API request URL for the first 100 ASINs\n\nvec_asin &lt;- lst_product_batches[[1]]$asin\n\nproduct_request_url &lt;- build_product_url(\n  api_key, domain_id, vec_asin,\n  offers = 20,\n  rating = 1,\n  buybox = 1\n  )\n\nThen, we query Keepa’s API by calling the defined URL:\n\nlst_products &lt;- get_product(product_request_url)\n\nThe response is a large list. If it is your first time downloading the data, please take some time to view the raw data through View(lst_products). The values are not necessarily self-explanatory (which is part of the reason for providing the keepar package). You can read the detailed explanation for each field in Keepa’s Product Object documentation.\nI recommend saving this raw response in .json format to your disk so that you can always return to the raw data later. We use the jsonlite package to save .json files.\n\njsonlite::write_json(\n  lst_products,\n  \"data/2-rawData/0001-raw-data.json\",\n  auto_unbox = TRUE\n)\n\nWe have now requested and saved data from 100 products. If that is sufficient, you can skip the following subsection and continue with Section 2.4.\n\n\n2.3.2 Requesting Data for More than 100 Products\nIf you require data on more than 100 products, you must request the data in batches of up to 100 products each. You can execute this request with the following loop, which automates the request timing. Once you have used up all tokens, the loop will automatically wait until your token balance is positive again. Only then will the script execute the subsequent request.\n\nfor (i in 1:length(lst_product_chunks)) {\n  \n  product_url &lt;- build_product_url(\n    api_key, domain_id, lst_product_batches[[i]]$asin,\n    offers = 20,\n    rating = 1,\n    buybox = 1\n  )\n  \n  # check if there are sufficient tokens, else wait\n  status &lt;- check_sufficient_tokens(api_key)\n  if (status$tokensBalance &lt; 1) {\n    message(\n      paste(\n        \"waiting\", round(status$secondsUntilTokensPositive / 60, 2),\n        \"minutes until\",\n        Sys.time() + status$secondsUntilTokensPositive + 60,\n        \"when token balance is positive again\"\n        )\n    )\n    Sys.sleep(status$secondsUntilTokensPositive + 60)\n  }\n  \n  # request data from API\n  lst_products &lt;- get_product(product_url)\n  \n  # save\n  jsonlite::write_json(\n    lst_products,\n    paste0(\n      \"data/2-rawData/\",\n      formatC(i, width = 4, format = \"d\", flag = \"0\"),\n      \"-raw-data.json\"\n    ),\n    auto_unbox = TRUE\n  )\n  \n  # print status\n  message(\n    paste0(\n      i, \"/\", length(lst_product_chunks), \" | \",\n      round(100 * i / length(lst_product_chunks), 0), \"% \",\n      \"of batches saved at \", Sys.time(),\n      \" | \", check_sufficient_tokens(api_key)$tokensBalance,\n      \" tokens left\")\n  )\n  \n}\n\nWe have now successfully saved all required raw data to our disk. However, we cannot do much with the raw .json files unless we transform them into an appropriate format for typical research methods."
  },
  {
    "objectID": "keepar.html#sec-transform-to-tidy",
    "href": "keepar.html#sec-transform-to-tidy",
    "title": "keepar",
    "section": "2.4 Transforming the Raw Data to Tidy Panel Data for our Analysis",
    "text": "2.4 Transforming the Raw Data to Tidy Panel Data for our Analysis\nTo transform the raw data, we first read it back into the workspace. Again, we use the purrr package and its map function to read all files into one list.\n\nvec_path_json &lt;- dir_ls(\"data/2-rawData/\") |&gt;\n  str_subset(\"json\")\nlst_json &lt;- map(vec_path_json, jsonlite::fromJSON)\n\nOnce we have loaded all the raw data into our workspace, we can extract the relevant data from it.\nWe do so with a suite of functions, explained in the following subsections. These subsections start with extracting the product metadata and then discuss the extraction of time-varying data, such as prices and sales ranks.\n\n2.4.1 Product Metadata\n\n2.4.1.1 tidy_product extracts most product-level metadata.\nThe function tidy_product generates a tidy data frame that includes most of the product metadata in the raw data. Every row corresponds to a single product, identified through its ASIN. This data frame includes, for example, information on the product title, its manufacturer, brand, and rootCategory (i.e., the broadest category of the product).\n\ndf_products &lt;- map_dfr(lst_json, tidy_product)\ndf_products\n#&gt; # A tibble: 247 x 54\n#&gt;    asin       title          domainId lastUpdate          imagesCSV manufacturer\n#&gt;    &lt;chr&gt;      &lt;chr&gt;             &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;     &lt;chr&gt;       \n#&gt;  1 B07L6BBW2Q Rubber Duck B~        1 2024-04-30 13:02:00 71wbjkVH~ LOUHUA      \n#&gt;  2 B07PB9GVPT Happy Trees D~        1 2024-04-30 13:02:00 511Wd7ip~ Lanterns ho~\n#&gt;  3 B07SMV3L4V 50-Pieces Flo~        1 2024-04-30 13:02:00 51Fm8Kle~ Guaren us   \n#&gt;  4 B007FPX8Z2 Rubber Duck N~        1 2024-04-30 13:02:00 41dQMuVN~ Duckshop    \n#&gt;  5 B01DW6S72Q Rubber Duck -~        1 2024-04-30 13:02:00 41DO0edx~ Duckshop    \n#&gt;  6 B07GMSSXN5 Teacher Rubbe~        1 2024-04-30 13:02:00 41fcj9xb~ Ducks in th~\n#&gt;  7 B008C2F0HQ Waddlers Rubb~        1 2024-04-30 13:02:00 61w5aV1h~ Assurance I~\n#&gt;  8 B00DS4EY4S Rubber Duck D~        1 2024-04-30 13:02:00 61avUzkp~ Duckshop    \n#&gt;  9 B07HCQTDK7 Golfer Rubber~        1 2024-04-30 13:02:00 71NYdXNO~ Schnabels   \n#&gt; 10 B00ILCA9G4 Dozen Classic~        1 2024-04-30 13:02:00 61e7+eEb~ &lt;NA&gt;        \n#&gt; # i 237 more rows\n#&gt; # i 48 more variables: lastPriceChange &lt;dttm&gt;, rootCategory &lt;dbl&gt;,\n#&gt; #   productType &lt;int&gt;, type &lt;chr&gt;, hasReviews &lt;lgl&gt;, trackingSince &lt;dttm&gt;,\n#&gt; #   brand &lt;chr&gt;, productGroup &lt;chr&gt;, partNumber &lt;chr&gt;, model &lt;chr&gt;,\n#&gt; #   color &lt;chr&gt;, size &lt;chr&gt;, packageHeight &lt;int&gt;, packageLength &lt;int&gt;,\n#&gt; #   packageWidth &lt;int&gt;, packageWeight &lt;int&gt;, packageQuantity &lt;int&gt;,\n#&gt; #   isAdultProduct &lt;lgl&gt;, isEligibleForTradeIn &lt;lgl&gt;, ...\n\n\n\n2.4.1.2 get_product_category_tree extracts products’ category trees\nEach product can be listed in multiple categories, organized in a hierarchical category tree. get_product_category_tree extracts this category tree:\n\ndf_category_tree &lt;- map_dfr(lst_json, get_product_category_tree)\ndf_category_tree\n#&gt; # A tibble: 739 x 4\n#&gt;    asin           catId catName             catDepth\n#&gt;    &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;                  &lt;int&gt;\n#&gt;  1 B07L6BBW2Q 165793011 Toys & Games               1\n#&gt;  2 B07L6BBW2Q 196601011 Baby & Toddler Toys        2\n#&gt;  3 B07L6BBW2Q 330390011 Bath Toys                  3\n#&gt;  4 B07PB9GVPT 165793011 Toys & Games               1\n#&gt;  5 B07PB9GVPT 196601011 Baby & Toddler Toys        2\n#&gt;  6 B07PB9GVPT 330390011 Bath Toys                  3\n#&gt;  7 B07SMV3L4V 165793011 Toys & Games               1\n#&gt;  8 B07SMV3L4V 196601011 Baby & Toddler Toys        2\n#&gt;  9 B07SMV3L4V 330390011 Bath Toys                  3\n#&gt; 10 B007FPX8Z2 165793011 Toys & Games               1\n#&gt; # i 729 more rows\n\nThis data frame contains the category’s ID (catId), name (catName), and depth (catDepth). The latter indicates a category’s position in the category tree: catDepth == 1 indicates a product’s broadest category. The higher this value, the narrower (i.e., more specific) the category.\nFor the first product in the data frame, our category tree appears as follows:\n\ndf_category_tree_first &lt;- df_category_tree |&gt; \n  filter(asin == \"B07L6BBW2Q\")\ndf_category_tree_first\n#&gt; # A tibble: 3 x 4\n#&gt;   asin           catId catName             catDepth\n#&gt;   &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;                  &lt;int&gt;\n#&gt; 1 B07L6BBW2Q 165793011 Toys & Games               1\n#&gt; 2 B07L6BBW2Q 196601011 Baby & Toddler Toys        2\n#&gt; 3 B07L6BBW2Q 330390011 Bath Toys                  3\n\nWe can see that the broadest category is Toys & Games, and the narrowest is Bath Toys. You can quickly access those categories through get_product_broadest_category and get_product_narrowest_category.\n\n\n\n2.4.2 Historical Event Data\nBeyond obtaining the current metadata, we are interested in the historical development of a product’s characteristics. We can obtain such data through the following functions.\nImportantly, all historical data is in an event data format. This format means that there will only be a new row in the data frame if the respective information has changed. This format differs from the typical data we use in social sciences, which is often evenly-spaced panel data with a single observation on each day or week. In contrast, event data features an irregularly spaced frequency and can have multiple observations per day or none for weeks, depending on how frequently the underlying product characteristics have changed.\n\n2.4.2.1 get_product_price_buybox extracts the product’s buy box price history\nEvery product on Amazon can have multiple sellers competing to sell the same product. Hence, every product can have various offers. Amazon simplifies the process of consumers picking a seller by recommending a default seller through the buy box. In analyses, it often makes sense to focus on the offer that wins the buy box because most customers buy from this recommended seller (Zeibak 2020; Lanxner 2019).\nWe now extract one of the most relevant historical product characteristics—the price. More precisely, this is the price of the offer occupying the buy box at the given time.\n\ndf_buybox_price &lt;- map_dfr(lst_json, get_product_price_buybox)\ndf_buybox_price\n#&gt; # A tibble: 20,465 x 4\n#&gt;    asin       datetime            priceBuybox shippingBuybox\n#&gt;    &lt;chr&gt;      &lt;dttm&gt;                    &lt;dbl&gt;          &lt;dbl&gt;\n#&gt;  1 B07L6BBW2Q 2019-03-27 23:34:00        13.0              0\n#&gt;  2 B07L6BBW2Q 2019-04-24 20:30:00        16.0              0\n#&gt;  3 B07L6BBW2Q 2019-04-27 02:04:00        NA               NA\n#&gt;  4 B07L6BBW2Q 2019-05-06 01:48:00        13.0              0\n#&gt;  5 B07L6BBW2Q 2019-05-14 08:36:00        12.0              0\n#&gt;  6 B07L6BBW2Q 2019-05-14 21:20:00        12.0              0\n#&gt;  7 B07L6BBW2Q 2019-05-18 14:42:00        12.0              0\n#&gt;  8 B07L6BBW2Q 2019-05-19 19:24:00        13.0              0\n#&gt;  9 B07L6BBW2Q 2019-05-27 18:52:00        13.0              0\n#&gt; 10 B07L6BBW2Q 2019-05-28 19:38:00        13.0              0\n#&gt; # i 20,455 more rows\n\nIf priceBuybox == NA, then the product did not have an active buy box seller at that particular time. That either means that the product was unavailable or that no seller qualified to win the buy box. Note that the resulting data frame includes two price columns: priceBuybox (the buy box price) and shippingBuybox (the shipping cost of the buy box offer). Often, shippingBuybox is zero. However, I recommend adding both to compute a price variable.\nWe can now visualize the development of a product’s prices over time. For simplicity, let us focus on the first rubber duck.\n\nvec_asin_filter &lt;- df_buybox_price |&gt;\n  distinct(asin) |&gt; slice(1) |&gt; pull()\n\nvec_title &lt;- df_products |&gt;\n  filter(asin %in% vec_asin_filter) |&gt; select(title) |&gt;\n  pull() |&gt; str_sub(1, 43)\n\ndf_buybox_price |&gt; \n  filter(asin %in% vec_asin_filter) |&gt; \n  mutate(price = priceBuybox + shippingBuybox) |&gt; \n  ggplot(aes(datetime, price)) +\n  geom_step() +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    title = paste0(\"Product: \", vec_title),\n    x = \"Date\",\n    y = \"Price\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 2: Buy Box Price Development of the First Rubber Duck in the Data Set\n\n\n\n\n\n\n\n\nFigure 2 already provides rich information on the product. We can see that the prices of rubber ducks vary strongly over time. While the cheapest available price over time is $7.99, the offer peaked at double this price ($15.99) for short durations.\nBecause the line indicating the price is missing for some dates, we can conclude that the product was not always available—if the price is missing, it implies that no seller won the buy box.\n\n\n2.4.2.2 get_product_buybox_history extracts the product’s buy box seller history\nYou can obtain information on which seller won the buy box at which time through get_product_buybox_history. Note that this function only works if you specify build_product_url(..., buybox = 1) such that the raw data includes information on the buy box history.\n\ndf_buybox_history &lt;- map_dfr(lst_json, get_product_buybox_history)\ndf_buybox_history\n#&gt; # A tibble: 11,932 x 3\n#&gt;    asin       datetime            buyBoxSeller        \n#&gt;    &lt;chr&gt;      &lt;dttm&gt;              &lt;chr&gt;               \n#&gt;  1 B07L6BBW2Q 2019-03-27 23:34:00 AC5QQUYS4DT2W       \n#&gt;  2 B07L6BBW2Q 2019-04-27 02:04:00 noSellerQualifiedBB \n#&gt;  3 B07L6BBW2Q 2019-05-06 01:48:00 AC5QQUYS4DT2W       \n#&gt;  4 B07L6BBW2Q 2019-05-14 21:20:00 A2PNVNIOI1WGC       \n#&gt;  5 B07L6BBW2Q 2019-05-18 14:42:00 AC5QQUYS4DT2W       \n#&gt;  6 B07L6BBW2Q 2019-05-27 18:52:00 A12SR7AJCMRNG9      \n#&gt;  7 B07L6BBW2Q 2019-05-30 04:54:00 noSellerOrOutOfStock\n#&gt;  8 B07L6BBW2Q 2019-06-05 09:24:00 AC5QQUYS4DT2W       \n#&gt;  9 B07L6BBW2Q 2019-06-27 15:58:00 A3G1S0O0LEO4WI      \n#&gt; 10 B07L6BBW2Q 2019-07-07 11:52:00 AC5QQUYS4DT2W       \n#&gt; # i 11,922 more rows\n\nIf you are interested in other prices, you can call get_product_price_amazon (for the price offered by Amazon as the seller) and get_product_price_marketplace (for the lowest non-Amazon offer).\n\n\n2.4.2.3 get_product_sales_rank_reference extracts the sales rank history for the reference category\nWe can obtain the sales rank history to analyze how well a product has sold. The sales rank is a proxy for sales based on the following simplified logic: Amazon ranks all products by sales for a specific product category. The best-selling product in the category receives the sales rank 1, the second-best-selling product receives sales rank 2, and so on. Hence, the absolute value of the sales rank is only comparable within the same category.\nAmazon provides—and Keepa tracks—different sales ranks for the same product. Often, the sales rank for a product’s reference category (also called root category) is useful—the reference category is the broadest available category. Hence, chances are highest that similar products fall into the same reference category, making the products’ sales ranks directly comparable.\nYou can extract the products’ sales ranks for the reference category through get_product_sales_rank_reference:\n\ndf_salesRank &lt;- map_dfr(lst_json, get_product_sales_rank_reference)\ndf_salesRank\n#&gt; # A tibble: 366,810 x 4\n#&gt;    asin       datetime            salesRankReference salesRank\n#&gt;    &lt;chr&gt;      &lt;dttm&gt;              &lt;chr&gt;                  &lt;int&gt;\n#&gt;  1 B07L6BBW2Q 2018-12-27 19:00:00 165793011            4140161\n#&gt;  2 B07L6BBW2Q 2018-12-29 22:50:00 165793011             264291\n#&gt;  3 B07L6BBW2Q 2019-01-01 19:52:00 165793011             173252\n#&gt;  4 B07L6BBW2Q 2019-01-10 09:50:00 165793011              56206\n#&gt;  5 B07L6BBW2Q 2019-01-11 03:02:00 165793011              32908\n#&gt;  6 B07L6BBW2Q 2019-01-12 15:36:00 165793011              58844\n#&gt;  7 B07L6BBW2Q 2019-01-13 18:56:00 165793011              94892\n#&gt;  8 B07L6BBW2Q 2019-01-14 01:10:00 165793011             126124\n#&gt;  9 B07L6BBW2Q 2019-01-15 13:12:00 165793011              48033\n#&gt; 10 B07L6BBW2Q 2019-01-17 04:24:00 165793011             130129\n#&gt; # i 366,800 more rows\n\nAs you can see from the number of observations in the resulting data frame, the sales rank data is comparatively abundant: 247 products yield 366,810 sales rank observations. Hence, it might be advisable to restrict your observation period to the smallest required time frame.\n\n\n2.4.2.4 get_product_rating extracts the products’ star rating history\nHow well did consumers like the product over time? You can answer this question by extracting the products’ rating history, which shows a product’s star rating at historical points in time.\n\ndf_rating &lt;- map_dfr(lst_json, get_product_rating)\ndf_rating\n#&gt; # A tibble: 1,937 x 3\n#&gt;    asin       datetime            rating\n#&gt;    &lt;chr&gt;      &lt;dttm&gt;               &lt;dbl&gt;\n#&gt;  1 B07L6BBW2Q 2019-03-27 23:34:00    5  \n#&gt;  2 B07L6BBW2Q 2019-04-16 03:12:00    4.5\n#&gt;  3 B07L6BBW2Q 2019-04-18 01:26:00    4.6\n#&gt;  4 B07L6BBW2Q 2019-05-06 01:48:00    4.7\n#&gt;  5 B07L6BBW2Q 2019-05-14 02:40:00    4.3\n#&gt;  6 B07L6BBW2Q 2019-05-19 19:24:00    4.2\n#&gt;  7 B07L6BBW2Q 2019-05-23 14:08:00    4.3\n#&gt;  8 B07L6BBW2Q 2019-05-28 19:38:00    4.4\n#&gt;  9 B07L6BBW2Q 2019-06-10 22:24:00    4.5\n#&gt; 10 B07L6BBW2Q 2019-06-23 06:12:00    4.1\n#&gt; # i 1,927 more rows\n\n\n\n2.4.2.5 get_product_reviews extracts the products’ history of number of reviews\nBeyond the star rating, you can also extract the number of reviews the given product had at past dates in its history. You can extract this historical information through get_product_reviews:\n\ndf_count_reviews &lt;- map_dfr(lst_json, get_product_reviews)\ndf_count_reviews\n#&gt; # A tibble: 9,640 x 3\n#&gt;    asin       datetime            countReviews\n#&gt;    &lt;chr&gt;      &lt;dttm&gt;                     &lt;int&gt;\n#&gt;  1 B07L6BBW2Q 2019-03-27 23:34:00            1\n#&gt;  2 B07L6BBW2Q 2019-04-16 03:12:00            2\n#&gt;  3 B07L6BBW2Q 2019-04-18 01:26:00            3\n#&gt;  4 B07L6BBW2Q 2019-05-06 01:48:00            4\n#&gt;  5 B07L6BBW2Q 2019-05-14 02:40:00            5\n#&gt;  6 B07L6BBW2Q 2019-05-19 19:24:00            6\n#&gt;  7 B07L6BBW2Q 2019-05-20 11:56:00            7\n#&gt;  8 B07L6BBW2Q 2019-05-23 14:08:00            8\n#&gt;  9 B07L6BBW2Q 2019-05-28 19:38:00            9\n#&gt; 10 B07L6BBW2Q 2019-05-30 04:54:00           10\n#&gt; # i 9,630 more rows\n\n\n\n\n2.4.3 Other functions\nThe keepar package contains some more functions to extract data from product objects. These include get_product_offers to check a product’s offers (i.e., which sellers have offered the product at least once) and get_product_offers_history to obtain the historical prices of those sellers’ offers."
  },
  {
    "objectID": "keepar.html#saving-product-data",
    "href": "keepar.html#saving-product-data",
    "title": "keepar",
    "section": "2.5 Saving Product Data",
    "text": "2.5 Saving Product Data\nOnce you have extracted all relevant product data, you can save those to your disk. I recommend doing so since the data might become larger than your available memory once you combine the individual data in the next section. We can follow our initial folder structure and create a new data/3-tidyData/ folder for the tidy-ed data.\nWe can either save the data individually by data frame:\n\nwrite_parquet(df_products, \"data/3-tidyData/df_products.parquet\")\n\nOr we can save them all at once:\n\n# define a list with all data frames to save\nlst(\n  df_products, df_category_tree, df_buybox_price,\n  df_buybox_history, df_salesRank, df_rating,\n  df_count_reviews\n) |&gt; \n  # save all data frames from the list individually\n  iwalk(\n    \\(x, y) write_parquet(\n      x, paste0(\"data/3-tidyData/\", y, \".parquet\")\n    )\n  )"
  },
  {
    "objectID": "keepar.html#sec-transform-to-panel",
    "href": "keepar.html#sec-transform-to-panel",
    "title": "keepar",
    "section": "2.6 Transforming Event to Panel Data",
    "text": "2.6 Transforming Event to Panel Data\nUntil now, we have extracted product meta or event data from the raw product data. This manner of storing the raw data is efficient—you only have to record a new row when a product attribute (e.g., its price) has changed. However, many typical methods—for example (fixed effects) regressions—require evenly-spaced (panel) data.\nThus, we must convert the unevenly-spaced event data set to an evenly-spaced panel data set. Also, we only have individual variables stored in individual data frames (e.g., prices and sales ranks are in two different data sets). Thus, we want to create a single panel data set that features a single observation per day per product, and which includes all previously introduced historical product variables (e.g., prices, sales ranks, and ratings).\nFirst, we read all previously saved data frames from the data/3-tidyData folder into a list called lst_df.\n\nvec_path &lt;- \"data/3-tidyData\"\n\nlst_df &lt;- dir_ls(vec_path) |&gt; \n  map(read_parquet) |&gt;\n  set_names(\n    basename(dir_ls(vec_path)) |&gt;\n    str_remove_all(\".parquet\")\n            )\n\n\n2.6.1 Interlude: A Note on Time Zones\nBefore we can start transforming the data—specifically, transforming the datetime variable from irregularly-spaced events to regularly-spaced daily frequency—we must consider time zones. All datetime values in the above data frames are in Coordinated Universal Time (UTC). For example:\n\ndf_rating |&gt; slice(1) |&gt; pull(datetime)\n#&gt; [1] \"2019-03-27 23:34:00 UTC\"\n\nIf you want to merge the above data with external data sources in local time, you need to transform the datetime variable. For example, the offset between UTC and Eastern Standard Time (EST) on the United States Amazon marketplace is -5 hours. We can transform the time zones with lubridate’s with_tz():\n\ndf_rating |&gt; slice(1) |&gt; pull(datetime) |&gt; with_tz(\"EST\")\n#&gt; [1] \"2019-03-27 18:34:00 EST\"\n\nIf we want to transform all datetime in the lst_df data frames into the EST time zone, we can run the following code across all elements of lst_df:\n\nlst_df &lt;- lst_df |&gt; \n  map(\n    \\(x) x |&gt; \n      mutate(\n        across(\n          where(is.POSIXct),\n          \\(y) with_tz(y, \"EST\")\n        )\n      )\n  )\n\n\n\n2.6.2 Generating a Daily Panel of a Single Variable\nLet us now start transforming the data from event to daily panel data. This subsection uses the buy box price data in df_buybox_price to explain the process and then provides the code for other variables.\n\n2.6.2.1 Buy Box Price\nFirst, however, we need to decide on which observation to use as the daily observation if there is more than one observation per day. Some options would be taking the mean, median, or the first or last price of the day. In the following, we take a cue from Jürgensmeier and Skiera (2024) and use the last daily observation to represent the daily value.\nWe start by extracting the last observation on each day for each product.\n\ndf_price_daily_raw &lt;- lst_df$df_buybox_price |&gt; \n  mutate(price = priceBuybox + shippingBuybox) |&gt; \n  mutate(date = as.Date(datetime),\n         isPriceNa = if_else(is.na(price), TRUE, FALSE)) |&gt; \n  group_by(asin, date) |&gt; \n  slice(n()) |&gt; # last observation: n(); first observation: 1\n  mutate(isPriceChange = TRUE)\n\nWe then create a grid data frame—containing one row for each product and each date—and left join it with df_price_daily_raw.\n\ndf_grid &lt;- df_price_daily_raw |&gt; \n  construct_ts_grid_by_group()\n\ndf_price_daily &lt;- df_grid |&gt; \n  left_join(df_price_daily_raw, by = c(\"asin\", \"date\")) |&gt;\n  fill(c(price , isPriceNa), .direction = \"down\") |&gt; \n  replace_na(list(isPriceChange = FALSE)) |&gt; \n  mutate(price = if_else(isPriceNa == TRUE, as.numeric(NA), price)) |&gt; \n  select(-c(datetime, priceBuybox, shippingBuybox))\n\nThis procedure yields a tidy panel data set with daily frequency including all buy box prices.\n\ndf_price_daily\n#&gt; # A tibble: 288,037 x 5\n#&gt;    date       asin       price isPriceNa isPriceChange\n#&gt;    &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;lgl&gt;     &lt;lgl&gt;        \n#&gt;  1 2017-05-26 B00001U0RG  9.99 FALSE     TRUE         \n#&gt;  2 2017-05-27 B00001U0RG  9.99 FALSE     FALSE        \n#&gt;  3 2017-05-28 B00001U0RG  9.99 FALSE     FALSE        \n#&gt;  4 2017-05-29 B00001U0RG  9.99 FALSE     FALSE        \n#&gt;  5 2017-05-30 B00001U0RG  9.99 FALSE     FALSE        \n#&gt;  6 2017-05-31 B00001U0RG  9.99 FALSE     FALSE        \n#&gt;  7 2017-06-01 B00001U0RG  9.99 FALSE     FALSE        \n#&gt;  8 2017-06-02 B00001U0RG  9.99 FALSE     FALSE        \n#&gt;  9 2017-06-03 B00001U0RG  9.99 FALSE     FALSE        \n#&gt; 10 2017-06-04 B00001U0RG  9.99 FALSE     FALSE        \n#&gt; # i 288,027 more rows\n\nNote that this data frame has one observation for each date and product—irrespective of whether df_price_daily_raw contained an observation for that day. In the above code, fill(..., .direction = \"down\") fills all NA values in a downward direction with the last non-NA value. This procedure is valid because Keepa only stores an entry in the event data sets if the underlying value (e.g., the price) changes. Thus, if there is no entry in the raw data, the price remains unchanged, and we can replace the NA value with the last known price. Similarly, if the buy box was unavailable, i.e., the price in df_price_daily_raw is NA, the above code ensures that the below values will remain NA until the buy box becomes available again.\n\n\n2.6.2.2 Sales Rank\nSimilar to the buy box price, we can transform the sales rank event to panel data through the following process:\n\ndf_sales_rank_daily_raw &lt;- lst_df$df_salesRank |&gt; \n  mutate(\n    date = as.Date(datetime),\n    isSalesRankNa = if_else(is.na(salesRank), TRUE, FALSE)\n    ) |&gt; \n  group_by(asin, date) |&gt; \n  slice(n())\n\ndf_salesrank_daily &lt;- df_sales_rank_daily_raw |&gt;\n  construct_ts_grid_by_group() |&gt; \n  left_join(df_sales_rank_daily_raw, by = c(\"asin\", \"date\")) |&gt;\n  fill(\n    c(salesRank, salesRankReference, isSalesRankNa),\n    .direction = \"down\"\n    ) |&gt;\n  mutate(salesRank = if_else(\n    isSalesRankNa == TRUE, as.integer(NA), salesRank)\n    ) |&gt; \n  select(-datetime)\n\n\n\n\n2.6.3 Generating a Daily Panel of Multiple Variables\nWe have now generated two panel data sets, one for the price variable and one for the salesRank variable. The following commands join those individual panels and generate a full panel data set that includes both variables.\n\ndf_panel_joined &lt;- df_salesrank_daily |&gt; \n  full_join(df_price_daily, by = c(\"asin\", \"date\"))\n\nYou can use the skim() function from the skimr package to easily generate the summary statistics for the resulting panel data set. This data set contains the daily information regarding prices and sales ranks for each product and enables us to use standard statistical methods, such as fixed-effects regression analyses, which often require evenly-spaced panel data.\n\nskimr::skim(df_panel_joined)\n\n\nData summary\n\n\nName\ndf_panel_joined\n\n\nNumber of rows\n458584\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nDate\n1\n\n\nlogical\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nasin\n0\n1.00\n10\n10\n0\n247\n0\n\n\nsalesRankReference\n36144\n0.92\n5\n9\n0\n2\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2015-02-01\n2024-04-30\n2020-11-11\n3377\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nisSalesRankNa\n12827\n0.97\n0.11\nFAL: 398498, TRU: 47259\n\n\nisPriceNa\n170547\n0.63\n0.13\nFAL: 250725, TRU: 37312\n\n\nisPriceChange\n170547\n0.63\n0.05\nFAL: 272977, TRU: 15060\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsalesRank\n60086\n0.87\n956370.58\n1369065.71\n446.00\n299205.0\n622698.00\n1131003.00\n13925552.0\n▇▁▁▁▁\n\n\nprice\n207859\n0.55\n15.31\n8.86\n2.44\n10.3\n13.89\n16.99\n124.1\n▇▁▁▁▁\n\n\n\n\n\nThese summary statistics conclude the section on identifying, obtaining, and transforming product data with the keepar package and lead us toward the data analysis."
  },
  {
    "objectID": "keepar.html#identify-christmas-themed-ducks",
    "href": "keepar.html#identify-christmas-themed-ducks",
    "title": "keepar",
    "section": "3.1 Identify Christmas-Themed Ducks",
    "text": "3.1 Identify Christmas-Themed Ducks\nBefore we can start this analysis, we have to identify all Christmas-themed rubber ducks. We do so by checking whether the title variable in the data frame df_products contains either the word Santa or Christmas (without case sensitivity).\n\ndf_products_isChristmasDuck &lt;- df_products |&gt; \n  mutate(\n    isChristmasDuck = if_else(\n      str_detect(\n        title, \"(?i)Santa|Christmas\"),\n      \"Christmas Ducks\", \"Other Ducks\"\n      )\n    ) |&gt; \n  select(asin, title, isChristmasDuck)\n\nOur data set of 247 contains nine Christmas-themed rubber ducks.\n\ndf_products_isChristmasDuck |&gt; \n  count(isChristmasDuck)\n#&gt; # A tibble: 2 x 2\n#&gt;   isChristmasDuck     n\n#&gt;   &lt;chr&gt;           &lt;int&gt;\n#&gt; 1 Christmas Ducks     9\n#&gt; 2 Other Ducks       238\n\nNow that we have identified whether each duck is Christmas-themed or not, we can visualize our historical product data set conditional on this information."
  },
  {
    "objectID": "keepar.html#visualize-sales-rank-development",
    "href": "keepar.html#visualize-sales-rank-development",
    "title": "keepar",
    "section": "3.2 Visualize Sales Rank Development",
    "text": "3.2 Visualize Sales Rank Development\nFor this analysis, we aim to visualize the sales rank development over time as a proxy for the products’ sales. Since we are interested in how well Christmas-themed rubber ducks sell compared to non-Christmas (i.e., “other”) rubber ducks, we visualize the mean daily sales rank for each of those two groups. Also, we generate a new variable isChristmasSeason, which indicates whether the observation is shortly before Christmas (i.e., between 2023-11-01 and 2023-12-25) or outside the Christmas season. The following code prepares the data accordingly and ensures that we include only the year 2023 for illustrative purposes.\n\ndf_panel_season_2023 &lt;- df_panel_joined |&gt; \n  left_join(df_products_isChristmasDuck, by = \"asin\") |&gt; \n  mutate(\n    isChristmasSeason = if_else(\n      date &gt;= as.Date(\"2023-11-01\") & date &lt;= as.Date(\"2023-12-25\"),\n      TRUE, FALSE\n      )\n    ) |&gt; \n  group_by(date, isChristmasDuck, isChristmasSeason) |&gt; \n  summarize(\n    salesRank = mean(salesRank, na.rm = TRUE)\n  ) |&gt; \n  filter(date &gt;= \"2023-01-01\" & date &lt;= \"2023-12-31\") |&gt; \n  ungroup() |&gt; \n  mutate(\n    isChristmasSeason = if_else(\n      isChristmasSeason, \"Christmas Season\",\n      \"Before Christmas Season\"\n      ),\n    isChristmasSeason = relevel(\n      as.factor(isChristmasSeason), ref = \"Before Christmas Season\"\n      ),\n    isChristmasDuck = as.factor(isChristmasDuck),\n    isChristmasDuck = relevel(\n      as.factor(isChristmasDuck), ref = \"Other Ducks\"\n      )\n  )\n\nFor the interpretation of the following analysis, remember that a higher sales rank means fewer sales. The best-selling product exhibits a sales rank of one within its category. The lower the sales of the product, the higher its sales rank. Thus, the sales rank decreases when the product sells better.\nFigure 3 plots the mean sales rank over time for Christmas-themed and other rubber ducks. The shaded area indicates the Christmas season in November and December, leading to Christmas Day on December 25. Based on Figure 3, sales of Christmas rubber ducks appear to be strongly seasonal. During the weeks before Christmas, the sales rank decreases substantially, which implies higher sales (note that the y-axis is inverted, such that a lower sales rank—reflecting more sales—appears higher).\nIn comparison, the sales rank of the other ducks decreases less strongly. Thus, both product groups seem to sell better during the Christmas period—compared to all other products in the same root category. But this increase in sales appears much stronger for Christmas-themed rubber ducks.\n\ndf_panel_season_2023 |&gt; \n  ggplot(aes(\n    date, salesRank,\n    color = isChristmasDuck, label = isChristmasDuck)\n    ) +\n  geom_line(linewidth = 1.2) +\n  geom_vline(\n    xintercept = as.Date(\"2023-12-25\"),\n    linetype = \"dashed\", linewidth = 1.2\n    ) +\n  annotate(geom = \"rect\",\n             xmin = as.Date(\"2023-11-01\"),\n             xmax = as.Date(\"2023-12-25\"),\n             ymin = -Inf,\n             ymax = +Inf,\n             alpha = 0.2) +\n  scale_y_reverse(label = scales::comma) +\n  scale_color_manual(values = c(\"darkgrey\", \"darkred\")) +\n  labs(\n    x = \"Date\",\n    y = \"Sales Rank\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_wrap(~isChristmasDuck, nrow = 2)\n\n\n\n\nFigure 3: Sales Rank Development of Christmas-Themed vs. Other Rubber Ducks Before and During Christmas Season"
  },
  {
    "objectID": "keepar.html#difference-in-differences-estimation",
    "href": "keepar.html#difference-in-differences-estimation",
    "title": "keepar",
    "section": "3.3 Difference-in-Differences Estimation",
    "text": "3.3 Difference-in-Differences Estimation\nWe now want to quantify the difference-in-differences between the sales rank of Christmas-themed vs. other rubber ducks before and during the Christmas period. Thus, we designate the Christmas-themed rubber ducks as the treatment and the other rubber ducks as the control group.\nBefore we estimate the corresponding regression model, we visualize the difference-in-differences between the treatment and control group before and during the Christmas season.\nBy comparing the respective mean sales ranks, Figure 4 shows that the sales rank of Christmas-themed rubber ducks decreases much more strongly (note the reversed y-axis) than the control group of other rubber ducks. Again, this visualization implies that Christmas-themed rubber ducks sell particularly well, even beyond the expected increase in sales for all rubber ducks during the Christmas season.\n\ndf_panel_season_2023 |&gt; \n  group_by(isChristmasDuck, isChristmasSeason) |&gt; \n  summarize(\n    salesRank = mean(salesRank), .groups = \"drop\"\n  ) |&gt; \n  ggplot(aes(x = isChristmasSeason, y = salesRank, color = isChristmasDuck, shape = isChristmasDuck)) +\n  geom_line(aes(group = isChristmasDuck), linewidth = 1) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"darkgrey\", \"darkred\")) +\n  scale_x_discrete(limit = c(\"Before Christmas Season\", \"Christmas Season\")) +\n  scale_y_reverse(label = scales::comma) +\n  labs(x = element_blank()) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    legend.position = \"top\",\n    legend.title = element_blank()\n  )\n\n\n\n\nFigure 4: Mean Sales Rank for Christmas-Themed vs. Other Rubber Ducks Before and During Christmas Season\n\n\n\n\n\n\n\n\nWe then estimate the difference-in-differences model with log(salesRank) as the dependent variable, and isChristmasDuck, isChristmasSeason, as well as their interaction, as independent variables. We take the natural logarithm of each sales rank because it enables a relative interpretation of changes in the sales rank such that we can report more intuitive percentage changes.\nMore precisely, we aim to estimate the following regression model:\n\\[\\begin{align}\n  log(\\mathit{salesRank}_{it}) = \\, &\\beta_0 + \\beta_1 \\, \\mathit{isChristmasDuck}_i + \\, \\\\\n  & \\beta_2 \\, \\mathit{isChristmasSeason}_t \\, + \\\\\n  & \\gamma \\, \\mathit{(isChristmasDuck}_i \\times \\mathit{isChristmasSeason}) + \\epsilon_{it}\n\\end{align}\\]\nWe can then interpret the estimated coefficient \\(\\hat\\gamma\\) of the interaction term of isChristmasDuck and isChristmasSeason as the incremental relative difference in salesRanks of Christmas vs. non-Christmas rubber ducks during the Christmas season.\nWe can estimate and visualize this regression analysis with the following code:\n\nlibrary(fixest)\n\nreg_1 &lt;- feols(\n  log(salesRank) ~ isChristmasDuck * isChristmasSeason,\n  data = df_panel_season_2023\n)\n\n\netable(\n  reg_1, markdown = TRUE,\n  dict = c(\n    isChristmasDuckChristmasDucks = \"Christmas Duck\",\n    isChristmasSeasonChristmasSeason = \"Christmas Season\"\n    )\n  )\n\n\n\n\nTable 2: Difference-in-Differences Estimates for the Impact of the Christmas Season on the Sales Ranks of Christmas-themed Rubber Ducks\n\n\n\n\n\n\nTable 2 displays the estimation results for the impact of the Christmas season on the sales rank of Christmas-themed rubber ducks in comparison to other rubber ducks.\nInterpreting the coefficient of the Christmas Duck x Christmas Season interaction (= -0.1260874), we see that Christmas-themed rubber ducks exhibit a lower sales rank by -11.85% (\\(=(exp(\\hat{\\beta})-1)\\times 100\\%\\)) compared to non-Christmas-themed rubber ducks during the Christmas season. This result confirms our intuition and the visual analysis of the data.\nWhile this analysis is relatively simple, it illustrates how researchers can use the acquired and transformed data to estimate standard econometric models."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "My teaching focuses on programming with R and applied econometrics in Marketing. I aim to convey a valuable data science toolkit that enables students to answer complex economic or business questions with data and statistics.\nTo convey the—sometimes technical and challenging—programming and statistical methods, I follow a teaching philosophy that avoids teacher’s monologues in lengthy lectures but instead forces students to get their hands dirty quickly and learn by doing. Hence, my courses involve short primers on technical concepts but heavily rely on the students working in small groups to solve small case studies. From my experience, students learn (sometimes challenging) topics such as programming and statistics in a more motivating and effective way.\nBelow, you will find my teaching portfolio and links to my teaching materials."
  },
  {
    "objectID": "teaching.html#course-objective",
    "href": "teaching.html#course-objective",
    "title": "Teaching",
    "section": "Course Objective",
    "text": "Course Objective\nMost academic fields require proficiency in at least one data-centered analysis tool. For many, the R programming language has become the tool of choice. However, the first steps in coding can be intimidating and discouraging—primarily if you have never worked with a programming language before. This course aims to provide a results-oriented, applied, and hands-on introduction to the essential parts of a Data Science project in R. We will introduce the libraries and frameworks necessary for your analysis and focus on teaching you the implementation and application of those tools. We’ll present small examples throughout the lecture and provide you with application exercises that you can work on for yourself.\nWe aim to show you the scope of possibilities within R and leave you with the impression that you can confidently implement your empirical projects in R. We will focus on the Tidyverse ecosystem, a consistent and intuitive framework for building your data analysis from start to finish. After completing this course, you know how to apply the essential Tidyverse tools for everyday Data Science tasks in R—primarily data wrangling, data visualization, and communicating results."
  },
  {
    "objectID": "teaching.html#course-description",
    "href": "teaching.html#course-description",
    "title": "Teaching",
    "section": "Course Description",
    "text": "Course Description\nWe aim this course at beginners who are either entirely new to R as a programming language and/or want to learn about the Tidyverse ecosystem. The course covers four primary areas of the typical data science process and introduces the respective tidyverse tools:\n\nPlotting with ggplot2\nData wrangling with dplyr\nCommunicating your results with R Markdown\nRegressions with tidymodels\n\nWe will not cover statistical or theoretical concepts in this course, as the focus will lie on applied coding."
  },
  {
    "objectID": "teaching.html#methods",
    "href": "teaching.html#methods",
    "title": "Teaching",
    "section": "Methods",
    "text": "Methods\nWe will let you eat cake first. What does that mean? Many programming courses start with the absolute basics — variable types, syntax, loops, etc. Those are important but quite dull in the beginning. Instead of monotonously walking you through those, we follow a different teaching philosophy.\nEach topic will start with a very friendly and sometimes a bit complicated cake. And you will dive right into it by executing and adapting the code for that “data science cake.”\nFor example, we will show you an advanced visualization right at the beginning of the course and focus on what is possible eventually. While this might appear intimidating at first (“how should I ever be able to code that from scratch?”), we will walk you through the steps and introduce the methods to get there during the course.\nThe course will alternate between short introductions to a concept or method and small do-it-yourself coding exercises. In between the three sessions, you are encouraged to work on provided exercises that further deepen your understanding."
  },
  {
    "objectID": "teaching.html#this-course-at-your-company-or-institution",
    "href": "teaching.html#this-course-at-your-company-or-institution",
    "title": "Teaching",
    "section": "This Course at Your Company or Institution?",
    "text": "This Course at Your Company or Institution?\nPlease contact me via E-Mail if you are interested in hosting this R course or a similar Python course at your institution.\n\n\n\nmoc.liamg@reiemsnegreuj.l"
  },
  {
    "objectID": "teaching.html#course-description-1",
    "href": "teaching.html#course-description-1",
    "title": "Teaching",
    "section": "Course Description",
    "text": "Course Description\nThis course teaches around 200 students per semester the basics of data analysis with the software R. We aim to enable the students with a valuable toolkit to answer data-driven economic questions with a modern Data Science toolkit. The course follows the Flipped Classroom principle.\nWe provide the course content in videos “on-demand” before the face-to-face classroom session. Students watch the videos before coming to class and hence learn theoretical concepts. We then build upon that foundation and use the face-to-face time in class to deepen students’ understanding. In particular, we fully allocate the time in class for interaction in the form of small 60-minutes hands-on data science exercises in small groups and effective Q&A sessions."
  },
  {
    "objectID": "teaching.html#course-description-2",
    "href": "teaching.html#course-description-2",
    "title": "Teaching",
    "section": "Course Description",
    "text": "Course Description\nThis course teaches students to analyze real-world banking customer data with common marketing analytics methods. We teach methodological skills, including regression-based (e.g., linear and logistic regressions) and machine learning methods (e.g., random forests), and their implementation in R. Beyond teaching technical skills, this course focuses on enabling students to make data-driven marketing decisions in a relevant business setting. We analyze real (anonymized) customer data from one of the largest German banks.\nWe organize the course along the customer life cycle:\n\nThe acquisition of new customers: Analysis of acquisition channels and their efficiency and optimization of the conversion “funnel.”\nThe development from one-off to serial customers: understanding cross-selling and optimizing sales in the customer base.\nThe collection and analysis of customer feedback to better understand customer satisfaction.\nThe analysis and prevention of cancellations (churn) to avoid cancellations and implement active cancellation prevention.\n\nThis course investigates these phases by empirically analyzing real banking customer data, including:\n\nThe exploration and preparation of data through exploratory data analysis and necessary data wrangling.\nThe analysis of log files and associated basic data analysis in e-commerce.\nThe analysis of purchase and cancellation behavior using suitable (regression-based) methods.\nThe analysis of customer feedback from web portals through basic text mining techniques."
  }
]